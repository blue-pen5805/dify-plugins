model: llama-3.3-70b
label:
  zh_Hans: llama-3.3-70b
  en_US: llama-3.3-70b
model_type: llm
features:
  - multi-tool-call
  - agent-thought
  - stream-tool-call
model_properties:
  mode: chat
  context_size: 8192
parameter_rules:
  - name: temperature
    use_template: temperature
  - name: top_p
    use_template: top_p
  - name: max_tokens
    use_template: max_tokens
    default: 1024
    min: 1
    max: 8192
  - name: seed
    label:
      en_US: Seed
    type: int
    help:
      en_US:
        If specified, model will make a best effort to sample deterministically,
        such that repeated requests with the same seed and parameters should return
        the same result. Determinism is not guaranteed, and you should refer to the
        system_fingerprint response parameter to monitor changes in the backend.
    required: false
  - name: response_format
    label:
      en_US: Response Format
    type: string
    help:
      en_US: specifying the format that the model must output
    required: false
    options:
      - text
      - json_object
pricing:
  input: "0.85"
  output: "1.20"
  unit: "0.000001"
  currency: USD
